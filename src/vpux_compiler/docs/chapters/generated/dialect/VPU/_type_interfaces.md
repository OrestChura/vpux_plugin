<!-- Autogenerated by mlir-tblgen; don't manually edit -->
# TypeInterface definitions
## ClusterTypeInterface (`ClusterTypeInterface`)

Interface for generating cluster-aware information for types.

### Methods:
#### `getPerClusterComputeShapes`

```c++
SmallVector<Shape> getPerClusterComputeShapes();
```
@brief Retrieve the array of compute shapes
@warning An important thing to consider with regards to compute shapes,
         is that modes like SEGMENTED and OVERLAPPED take precedence over
         DUPLICATED and MULTICASTED.
         In an example case of a "SEGMENTED | DUPLICATED" (needed for SplitOverK)
         tensor with shape [1, 64, 4, 4], the compute shape in each cluster is
         [1, 16, 4, 4], which is needed when tiling and generating workloads,
         while the allocated shape is [1, 64, 4, 4] (because of duplicated)
         information which is needed for scheduler and strategy manager,
         in order to estimate memory
         In an example of OVERLAPPED over H with uniform segmentation for
         4 clusters, a tensor of shape [1, 64, 22, 16] will have the following
         compute distribution across clusters:
         [1 64 6 16] [1 64 6 16] [1 64 5 16] [1 64 5 16]

NOTE: This method *must* be implemented by the user.

#### `getPerClusterComputeShapeOffsets`

```c++
SmallVector<Shape> getPerClusterComputeShapeOffsets();
```
@brief Retrieve the array of compute shape offsets with regards to the full buffer
@warning An important thing to consider with regards to compute offsets,
         is that modes like SEGMENTED and OVERLAPPED take precedence over
         DUPLICATED and MULTICASTED.

NOTE: This method *must* be implemented by the user.

#### `getPerClusterMemoryShapes`

```c++
SmallVector<Shape> getPerClusterMemoryShapes();
```
@brief Retrieve the array of memory shapes
@warning An important thing to consider with regards to memory shapes,
         is that modes like DUPLICATED and MULTICASTED take precedence over
         SEGMENTED and OVERLAPPED.
         In an example case of a "SEGMENTED | DUPLICATED" (needed for SplitOverK)
         tensor with shape [1, 64, 4, 4], the memory shape in each cluster is
         [1, 64, 4, 4], which is the allocated shape (because of duplicated)
         information which is needed for scheduler and strategy manager,
         in order to estimate memory
         In an example of OVERLAPPED over H with k3x3s1 pad (1, 1, 1, 1) and
         uniform segmentation across 4 clusters, a tensor of shape [1, 64, 22, 16]
         will have the following memory distribution across clusters:
         [1 64 7 16] [1 64 8 16] [1 64 7 16] [1 64 6 16]

NOTE: This method *must* be implemented by the user.

#### `getPerClusterMemoryShapeOffsets`

```c++
SmallVector<Shape> getPerClusterMemoryShapeOffsets();
```
@brief Retrieve the array of memory shape offsets with regards to the full buffer
@warning An important thing to consider with regards to memory shape offsets,
         is that modes like DUPLICATED and MULTICASTED take precedence over
         SEGMENTED and OVERLAPPED.

NOTE: This method *must* be implemented by the user.

#### `getLargestCompactShape`

```c++
Shape getLargestCompactShape();
```
@brief Get largest compact compute shape
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getCompactShape`

```c++
Shape getCompactShape(int64_t tileInd);
```
@brief Get the compact compute shape for a specific cluster
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getPerClusterPadding`

```c++
SmallVector<vpux::PadInfo> getPerClusterPadding();
```
@brief Retrieve the array of padding for each cluster
@warning This function is needed for getting padding in OVERLAPPED mode

NOTE: This method *must* be implemented by the user.

#### `getPerClusterMemoryStridedShapes`

```c++
SmallVector<StridedShape> getPerClusterMemoryStridedShapes();
```
@brief Retrieve the array of strided compute shapes
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getLargestStridedShape`

```c++
StridedShape getLargestStridedShape();
```
@brief Get largest strided compute shape
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

#### `getStridedShape`

```c++
StridedShape getStridedShape(int64_t tileInd);
```
@brief Get the strided compute shape for a specific cluster
@warning This function should not be used for memory size calculation,
         because it does not retrieve the true allocate shape in cases
         of broadcasting.

NOTE: This method *must* be implemented by the user.

## DistributedTypeInterface (`VPU_DistributedTypeInterface`)

Interface for types that work with distributed components.
It is compatible with types that containg multiple types internally.

### Methods:
#### `containsDistributedTypes`

```c++
bool containsDistributedTypes();
```
Returns true if the components are distributed types
NOTE: This method *must* be implemented by the user.

#### `getDistributedTypes`

```c++
SmallVector<mlir::Type> getDistributedTypes();
```
Returns the distributed components
NOTE: This method *must* be implemented by the user.

